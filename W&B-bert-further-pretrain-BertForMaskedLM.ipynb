{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bd66b12-3efc-49a7-bdeb-739ff11bd456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:../log/bwdataset_2022-04-18.log\n",
      "logfilepath:../log/qnadataset_2022-04-18.log\n"
     ]
    }
   ],
   "source": [
    "# MLM 방식을 이용한 Further pre-traning 방식 구현 예제\n",
    "# 참고 소스 : https://towardsdatascience.com/masked-language-modelling-with-bert-7d49793e5d2c 참조 바람\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizer, BertConfig, BertForMaskedLM, BertTokenizerFast\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from myutils import GPU_info, seed_everything, mlogging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7837f0b-29db-43f6-a649-c3608d2417ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkobongsoo\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#======================================================================================================\n",
    "# *WandB 툴을 이용하여, HyperPrameter sweeps 하는 예제임(http://wandb.ai 에 회원가입 해야 함)\n",
    "#  \n",
    "# == HyperPrameter sweeps 과정 ==\n",
    "#\n",
    "# 1. 로그인\n",
    "# import wandb\n",
    "# wandb.login()  # 로그인 => wandb.ai 사이트에 로그인 후, API Key 입력해야 함\n",
    "#\n",
    "# 2. sweep_config 설정\n",
    "# sweep_config ={ \n",
    "#       'method': 'random', #grid, random\n",
    "#       'metric':{\n",
    "#           'name': 'val_accuracy', #loss\n",
    "#           'goal': 'maximize'      # loss일때는 minimize로 설정해야함\n",
    "#       },\n",
    "#      'parameters':{\n",
    "#           'learning_rate':{\n",
    "#               'distribution': 'uniform',\n",
    "#               'min' : 0,\n",
    "#               'max': 7e-5\n",
    "#           },\n",
    "#           'batch_size':{\n",
    "#               'values' : [8, 16, 32]\n",
    "#           },\n",
    "#           'epochs':{\n",
    "#               'values' : [2, 3, 4]\n",
    "#           }\n",
    "#       }\n",
    "#}\n",
    "#\n",
    "#sweep_id = wandb.sweep(sweep_config, project=\"bert-wb-test\")\n",
    "#\n",
    "# 3. wandb 초기화 \n",
    "# - wandb.init(config=config)\n",
    "#\n",
    "# 4. wandb 로그설정\n",
    "# - wandb.log({\"val_accuracy\": total_test_correct / total_test_len})\n",
    "#\n",
    "# 5. wand 실행\n",
    "# - wandb.agent(sweep_id, train, count=3)\n",
    "#\n",
    "# 6. wand 종료\n",
    "# -wandb.finish()\n",
    "#======================================================================================================\n",
    "# 1. 로그인\n",
    "#!pip install wandb -qqq\n",
    "import wandb\n",
    "wandb.login()  # 로그인 => wandb.ai 사이트에 로그인 후, API Key 입력해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b08e24a-e5e0-4ae5-9bc0-9c39a93a6a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: d31ha4x0\n",
      "Sweep URL: https://wandb.ai/kobongsoo/bert-wb-mlm-test-2/sweeps/d31ha4x0\n"
     ]
    }
   ],
   "source": [
    "# 2. sweep_config 설정\n",
    "\n",
    "sweep_config ={ \n",
    "      'method': 'random', #grid, random\n",
    "      'metric':{\n",
    "          'name': 'val_accuracy', #loss\n",
    "          'goal': 'maximize'      # loss일때는 minimize로 설정해야함\n",
    "      },\n",
    "     'parameters':{\n",
    "          'learning_rate':{\n",
    "              'distribution': 'uniform',\n",
    "              'min' :1e-5,\n",
    "              'max': 7e-5\n",
    "          },\n",
    "          'batch_size':{\n",
    "              'values' : [16, 32]\n",
    "          },\n",
    "          'epochs':{\n",
    "              'values' : [3, 4, 5]\n",
    "          }\n",
    "      }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"bert-wb-mlm-test-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3321a0d-57d3-400b-be05-619e1bf63842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n",
      "logfilepath:../log/wb-bert-fpt-mlm_2022-04-18.log\n"
     ]
    }
   ],
   "source": [
    "# 훈련시킬 말뭉치(사전 만들때 동일한 말뭉치 이용)\n",
    "input_corpus = \"../korpora/kowiki_20190620/wiki_20190620_small.txt\"\n",
    "\n",
    "# eval 말뭉치 \n",
    "eval_corpus = \"../korpora/kowiki_20190620/wiki_eval_test.txt\"\n",
    "\n",
    "# 기존 사전훈련된 모델\n",
    "model_path = \"../model/bert/bmc-fpt-wiki_20190620_mecab_false_0311-nouns-0416/\"\n",
    "\n",
    "# 기존 사전 + 추가된 사전 파일\n",
    "#vocab_path=\"tokenizer/wiki_20190620_false_0311_speical/bmc_add_wiki_20190620_false_0311.txt\"\n",
    "vocab_path=\"../model/bert/bmc-fpt-wiki_20190620_mecab_false_0311-nouns-0416/\"\n",
    "\n",
    "# 출력\n",
    "OUTPATH = '../model/bert/bmc-fpt-wiki_20190620_mecab_false_0311-nouns-0418/'\n",
    "\n",
    "token_max_len = 128\n",
    "\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(222)\n",
    "\n",
    "#logging 설정\n",
    "logger =  mlogging(loggername=\"wb-bert-fpt-mlm\", logfilename=\"../log/wb-bert-fpt-mlm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd6fad4a-7b5b-4502-93bc-7564f197398e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "special_token_size: 27, tokenizer.vocab_size: 167537\n",
      "vocab_size: 167560\n",
      "tokenizer_len: 167550\n"
     ]
    }
   ],
   "source": [
    "# tokeinzier 생성\n",
    "# tokenizer 생성\n",
    "# => BertTokenizer, BertTokenizerFast 둘중 사용하면됨\n",
    "\n",
    "#tokenizer = BertTokenizer(vocab_file=vocab_path, max_len=token_max_len, do_lower_case=False)\n",
    "tokenizer = BertTokenizer.from_pretrained(vocab_path, max_len=token_max_len, do_lower_case=False)\n",
    "# tokenizer = BertTokenizerFast(vocab_file=vocab_file, max_len=token_max_len, do_lower_case=False)\n",
    "\n",
    "\n",
    "# speical 토큰 계수 + vocab 계수 - 이미 vocab에 포함된 speical 토큰 계수(5)\n",
    "vocab_size = len(tokenizer.all_special_tokens) + tokenizer.vocab_size - 5 + 1\n",
    "#vocab_size = len(tokenizer.all_special_tokens) + tokenizer.vocab_size - 5\n",
    "print('special_token_size: {}, tokenizer.vocab_size: {}'.format(len(tokenizer.all_special_tokens), tokenizer.vocab_size))\n",
    "print('vocab_size: {}'.format(vocab_size))\n",
    "print('tokenizer_len: {}'.format(len(tokenizer)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2bfa9e0-34ad-4c1d-95b9-8fc65cfc0923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    # 모델 로딩 further pre-training \n",
    "    #config = BertConfig.from_pretrained(model_path)\n",
    "    #model = BertForMaskedLM.from_pretrained(model_path, from_tf=bool(\".ckpt\" in model_path), config=config) \n",
    "    model = BertForMaskedLM.from_pretrained(model_path)    \n",
    "\n",
    "    #################################################################################\n",
    "    # 모델 embedding 사이즈를 tokenizer 크기 만큼 재 설정함.\n",
    "    # 재설정하지 않으면, 다음과 같은 에러 발생함\n",
    "    # CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)` CUDA 에러가 발생함\n",
    "    #  indexSelectLargeIndex: block: [306,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
    "    #\n",
    "    #     해당 오류는 기존 Embedding(8002, 768, padding_idx=1) 처럼 입력 vocab 사이즈가 8002인데,\n",
    "    #     0~8001 사이를 초과하는 word idx 값이 들어가면 에러 발생함.\n",
    "    #################################################################################\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e41cf1f-4010-4348-8921-67d0f21ea09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(batch_size):\n",
    "    from torch.utils.data import DataLoader, RandomSampler\n",
    "    import sys\n",
    "    sys.path.append('..')\n",
    "    from myutils import MLMDataset\n",
    "  \n",
    "    # 각 스페셜 tokenid를 구함\n",
    "    CLStokenid = tokenizer.convert_tokens_to_ids('[CLS]')\n",
    "    SEPtokenid = tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "    UNKtokenid = tokenizer.convert_tokens_to_ids('[UNK]')\n",
    "    PADtokenid = tokenizer.convert_tokens_to_ids('[PAD]')\n",
    "    MASKtokenid = tokenizer.convert_tokens_to_ids('[MASK]')\n",
    "    print('CLSid:{}, SEPid:{}, UNKid:{}, PADid:{}, MASKid:{}'.format(CLStokenid, SEPtokenid, UNKtokenid, PADtokenid, MASKtokenid))\n",
    "\n",
    "    #===============================================================================\n",
    "    # 학습 dataloader 생성\n",
    "    train_dataset = MLMDataset(corpus_path = input_corpus,\n",
    "                               tokenizer = tokenizer, \n",
    "                               CLStokeinid = CLStokenid ,   # [CLS] 토큰 id\n",
    "                               SEPtokenid = SEPtokenid ,    # [SEP] 토큰 id\n",
    "                               UNKtokenid = UNKtokenid ,    # [UNK] 토큰 id\n",
    "                               PADtokenid = PADtokenid,    # [PAD] 토큰 id\n",
    "                               Masktokenid = MASKtokenid,   # [MASK] 토큰 id\n",
    "                               max_sequence_len=token_max_len,  # max_sequence_len)\n",
    "                               mlm_probability=0.15,\n",
    "                               overwrite_cache=False\n",
    "                              )\n",
    "\n",
    "\n",
    "    # 학습 dataloader 생성\n",
    "    # => tenosor로 만듬\n",
    "    train_loader = DataLoader(train_dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              #shuffle=True, # dataset을 섞음\n",
    "                              sampler=RandomSampler(train_dataset, replacement=False), #dataset을 랜덤하게 샘플링함\n",
    "                              num_workers=3\n",
    "                             )\n",
    "    #===============================================================================\n",
    "\n",
    "    #===============================================================================\n",
    "    # eval dataloader 생성\n",
    "    eval_dataset = MLMDataset(corpus_path = eval_corpus,\n",
    "                               tokenizer = tokenizer, \n",
    "                               CLStokeinid = CLStokenid ,   # [CLS] 토큰 id\n",
    "                               SEPtokenid = SEPtokenid ,    # [SEP] 토큰 id\n",
    "                               UNKtokenid = UNKtokenid ,    # [UNK] 토큰 id\n",
    "                               PADtokenid = PADtokenid,    # [PAD] 토큰 id\n",
    "                               Masktokenid = MASKtokenid,   # [MASK] 토큰 id\n",
    "                               max_sequence_len=token_max_len,  # max_sequence_len)\n",
    "                               mlm_probability=0.15,\n",
    "                               overwrite_cache=False\n",
    "                              )\n",
    "\n",
    "\n",
    "    # eval dataloader 생성\n",
    "    # => tenosor로 만듬\n",
    "    eval_loader = DataLoader(eval_dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              #shuffle=True, # dataset을 섞음\n",
    "                              sampler=RandomSampler(eval_dataset, replacement=False), #dataset을 랜덤하게 샘플링함\n",
    "                              num_workers=3\n",
    "                             )\n",
    "     #===============================================================================\n",
    "\n",
    "    #print(train_dataset[0])\n",
    "    \n",
    "    return train_loader, eval_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59891576-ff4e-4826-b5b3-90468005a80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def save_model(model, tokenizer, OUT_PATH, config):\n",
    "    \n",
    "     # 현재 local 시간 얻어옴(20220414-12-20)\n",
    "    tm = time.localtime(time.time())  \n",
    "    tt = f\"batch:{config.batch_size}-ep:{config.epochs}-lr:{config.learning_rate:.9f}-{tm.tm_mon}m{tm.tm_mday}d-{tm.tm_hour}:{tm.tm_min}\"\n",
    "                \n",
    "    TMP_OUT_PATH = OUTPATH + tt\n",
    "    \n",
    "    ### 전체모델 저장\n",
    "    os.makedirs(TMP_OUT_PATH, exist_ok=True)\n",
    "    #torch.save(model, OUTPATH + 'pytorch_model.bin') \n",
    "    # save_pretrained 로 저장하면 config.json, pytorch_model.bin 2개의 파일이 생성됨\n",
    "    model.save_pretrained(TMP_OUT_PATH)\n",
    "\n",
    "    # tokeinizer 파일 저장(vocab)\n",
    "    VOCAB_PATH = TMP_OUT_PATH\n",
    "    tokenizer.save_pretrained(VOCAB_PATH)\n",
    "    \n",
    "    logger.info(f'==> save_model : {TMP_OUT_PATH}')\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89756e9c-7002-4d63-b2ef-a3431486fb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_epoch(config,\n",
    "                model,\n",
    "                train_loader,\n",
    "                eval_loader):\n",
    "    \n",
    "    ##################################################\n",
    "    epochs = config.epochs            # epochs\n",
    "    learning_rate = config.learning_rate  # 학습률\n",
    "    #p_itr = 15000           # 손실률 보여줄 step 수\n",
    "    #save_steps = 50000     # 50000 step마다 모델 저장\n",
    "    ##################################################\n",
    "\n",
    "    # optimizer 적용\n",
    "    optimizer = AdamW(model.parameters(), \n",
    "                     lr=learning_rate, \n",
    "                     eps=1e-8) # 0으로 나누는 것을 방지하기 위한 epsilon 값(10^-6 ~ 10^-8 사이 이값 입력합)\n",
    "\n",
    "    # 총 훈련과정에서 반복할 스탭\n",
    "    total_steps = len(train_loader)*epochs\n",
    "    warmup_steps = total_steps * 0.1 #10% of train data for warm-up\n",
    "    \n",
    "    # 손실률 보여줄 step 수\n",
    "    p_itr = int(len(train_loader)*0.1)  \n",
    "    \n",
    "    # step마다 모델 저장\n",
    "    save_steps = int(total_steps * 0.5)\n",
    "    \n",
    "    # 스캐줄러 생성\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps=warmup_steps, \n",
    "                                                num_training_steps=total_steps)\n",
    "\n",
    "    itr = 1\n",
    "    total_loss = 0\n",
    "    total_len = 0\n",
    "    total_correct = 0\n",
    "    total_test_correct = 0\n",
    "    total_test_len = 0\n",
    "            \n",
    "    list_train_loss = []\n",
    "    list_train_acc = []\n",
    "    list_validation_acc = []\n",
    "\n",
    "    model.zero_grad()# 그래디언트 초기화\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "\n",
    "        # model.train() # 훈련모드로 변환\n",
    "        for data in tqdm(train_loader):\n",
    "            model.train() # 훈련모드로 변환\n",
    "            #optimizer.zero_grad()\n",
    "            model.zero_grad()# 그래디언트 초기화\n",
    "\n",
    "            # 입력 값 설정\n",
    "            input_ids = data['input_ids'].to(device)\n",
    "            attention_mask = data['attention_mask'].to(device)\n",
    "            token_type_ids = data['token_type_ids'].to(device)       \n",
    "            labels = data['labels'].to(device)\n",
    "            #print('Labels:{}'.format(labels))\n",
    "\n",
    "            # 모델 실행\n",
    "            outputs = model(input_ids=input_ids, \n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            labels=labels)\n",
    "\n",
    "            # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            #print('Loss:{}, logits:{}'.format(loss, logits))\n",
    "\n",
    "            # optimizer 과 scheduler 업데이트 시킴\n",
    "            loss.backward()   # backward 구함\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   # 그래디언트 클리핑 (gradient vanishing이나 gradient exploding 방지하기 위한 기법)\n",
    "            optimizer.step()  # 가중치 파라미터 업데이트(optimizer 이동)\n",
    "            scheduler.step()  # 학습률 감소\n",
    "\n",
    "            # ***further pretrain 에는 손실률 계산을 넣지 않음\n",
    "            # 정확도 계산하는 부분은 no_grade 시켜서, 계산량을 줄임.\n",
    "\n",
    "            # => torch.no_grad()는 gradient을 계산하는 autograd engine를 비활성화 하여 \n",
    "            # 필요한 메모리를 줄이고, 연산속도를 증가시키는 역활을 함\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                # 손실(loss) 계산 \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                #===========================================\n",
    "                # 정확도(Accurarcy) 계산\n",
    "                pred = torch.argmax(logits, dim=2)\n",
    "                tmpcorrect = pred.eq(labels)\n",
    "\n",
    "                # 예측값 중 true인값 * attention_maks가 = 1(True)인것 중에서 True값이 합이 masked에서 알아맞춘 단어 계수임\n",
    "                correct = tmpcorrect*attention_mask \n",
    "                total_correct += correct.sum().item() \n",
    "\n",
    "                # 단어 총 수는 attension_mask가 1(True) 인 것들의 합\n",
    "                total_len += attention_mask.sum().item() \n",
    "                #=========================================   \n",
    "    \n",
    "                # 주기마다 test(validataion) 데이터로 평가하여 손실류 계산함.\n",
    "                if itr % p_itr == 0:\n",
    "                    train_loss = total_loss/p_itr\n",
    "                    train_acc = total_correct/total_len\n",
    "                         \n",
    "                    ####################################################################\n",
    "                    # 주기마다 eval(validataion) 데이터로 평가하여 손실류 계산함.\n",
    "                    # 평가 시작\n",
    "                    model.eval()\n",
    "\n",
    "                    #for data in tqdm(eval_loader):\n",
    "                    for data in eval_loader:\n",
    "                        # 입력 값 설정\n",
    "                        input_ids = data['input_ids'].to(device)\n",
    "                        attention_mask = data['attention_mask'].to(device)\n",
    "                        token_type_ids = data['token_type_ids'].to(device)       \n",
    "                        labels = data['labels'].to(device)\n",
    "\n",
    "                        # 손실률 계산하는 부분은 no_grade 시켜서, 계산량을 줄임.\n",
    "                        # => torch.no_grad()는 gradient을 계산하는 autograd engine를 비활성화 하여 \n",
    "                        # 필요한 메모리를 줄이고, 연산속도를 증가시키는 역활을 함\n",
    "                        with torch.no_grad():\n",
    "                            # 모델 실행\n",
    "                            outputs = model(input_ids=input_ids, \n",
    "                                            attention_mask=attention_mask,\n",
    "                                            token_type_ids=token_type_ids,\n",
    "                                            labels=labels)\n",
    "\n",
    "                            # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "                            #loss = outputs.loss\n",
    "                            logits = outputs.logits\n",
    "\n",
    "                             #===========================================\n",
    "                            # 정확도(Accurarcy) 계산\n",
    "                            pred = torch.argmax(logits, dim=2)\n",
    "                            tmpcorrect = pred.eq(labels)\n",
    "\n",
    "                            # 예측값 중 true인값 * attention_maks가 = 1(True)인것 중에서 True값이 합이 masked에서 알아맞춘 단어 계수임\n",
    "                            correct = tmpcorrect*attention_mask \n",
    "                            total_test_correct += correct.sum().item() \n",
    "\n",
    "                            # 단어 총 수는 attension_mask가 1(True) 인 것들의 합\n",
    "                            total_test_len += attention_mask.sum().item() \n",
    "                            #========================================= \n",
    "\n",
    "                    val_acc = total_test_correct/total_test_len\n",
    "                    \n",
    "                    logger.info('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Train Acc: {:.4f}, Val Acc:{}'.format(epoch+1, epochs, itr, train_loss, train_acc, val_acc))\n",
    "                    \n",
    "                    list_train_loss.append(train_loss)\n",
    "                    list_train_acc.append(train_acc)\n",
    "                    list_validation_acc.append(val_acc)\n",
    "                 \n",
    "                    \n",
    "                    # wandb 로그 기록\n",
    "                    wandb.log({\"Loss\": train_loss,\n",
    "                              \"train_accuracy\": train_acc,\n",
    "                              \"val_accuracy\": val_acc\n",
    "                              })\n",
    "                 \n",
    "                    total_loss = 0\n",
    "                    total_len = 0\n",
    "                    total_correct = 0\n",
    "                    total_test_correct = 0\n",
    "                    total_test_len = 0\n",
    "                    ####################################################################\n",
    "\n",
    "                  # 모델 저장\n",
    "                if itr % save_steps == 0:\n",
    "                    save_model(model, tokenizer, OUTPATH, config)\n",
    "                    \n",
    "            itr+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09742433-658c-4fec-9ea3-bea2ad09a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련\n",
    "def train(config=None):\n",
    "    # wandb 초기화\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        \n",
    "        # 데이터 로더 생성\n",
    "        train_loader, eval_loader = build_dataset(config.batch_size)\n",
    "        \n",
    "        # 모델 로딩\n",
    "        tmodel = load_model()\n",
    "        \n",
    "        # 훈련 시작 \n",
    "        Train_epoch(config, tmodel, train_loader, eval_loader)\n",
    "        \n",
    "        # 모델 저장\n",
    "        #save_model(tmodel, tokenizer, OUTPATH, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa04ae9e-a88f-441b-9af4-dcff61530a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6g0qns8c with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 4.151229402443651e-06\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/MOCOMSYS/dev/bong/WandB/wandb/run-20220418_131518-6g0qns8c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kobongsoo/bert-wb-mlm-test-2/runs/6g0qns8c\" target=\"_blank\">driven-sweep-3</a></strong> to <a href=\"https://wandb.ai/kobongsoo/bert-wb-mlm-test-2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/kobongsoo/bert-wb-mlm-test-2/sweeps/d31ha4x0\" target=\"_blank\">https://wandb.ai/kobongsoo/bert-wb-mlm-test-2/sweeps/d31ha4x0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 13:15:24,533 - bwpdataset - INFO - ==>[Start] cached file read: ../korpora/kowiki_20190620/cached_lm_BertTokenizer_128_wiki_20190620_small.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLSid:101, SEPid:102, UNKid:100, PADid:0, MASKid:103\n",
      "*corpus:../korpora/kowiki_20190620/wiki_20190620_small.txt\n",
      "*max_sequence_len:128\n",
      "*mlm_probability:0.15\n",
      "*CLStokenid:101, SEPtokenid:102, UNKtokenid:100, PADtokeinid:0, Masktokeid:103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 13:15:24,750 - bwpdataset - INFO - <==[End] Loading features from cached file ../korpora/kowiki_20190620/cached_lm_BertTokenizer_128_wiki_20190620_small.txt [took 0.215 s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*corpus:../korpora/kowiki_20190620/wiki_eval_test.txt\n",
      "*max_sequence_len:128\n",
      "*mlm_probability:0.15\n",
      "*CLStokenid:101, SEPtokenid:102, UNKtokenid:100, PADtokeinid:0, Masktokeid:103\n",
      "*total_line: 114\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0975416024984360ba2536d08b7131b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d6e0fa9dfa44d68bc193d85021c277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d38c4700bab4344a050beb1700c0cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6243c4db67454ac885be4c309abdee9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 13:15:37,683 - wb-bert-fpt-mlm - INFO - [Epoch 1/5] Iteration 30 -> Train Loss: 0.4533, Train Acc: 0.8044, Val Acc:0.8234729772945315\n",
      "2022-04-18 13:15:46,694 - wb-bert-fpt-mlm - INFO - [Epoch 1/5] Iteration 60 -> Train Loss: 0.4479, Train Acc: 0.8003, Val Acc:0.8241125679565078\n",
      "2022-04-18 13:15:55,754 - wb-bert-fpt-mlm - INFO - [Epoch 1/5] Iteration 90 -> Train Loss: 0.4208, Train Acc: 0.8050, Val Acc:0.8279501119283659\n",
      "2022-04-18 13:16:04,836 - wb-bert-fpt-mlm - INFO - [Epoch 1/5] Iteration 120 -> Train Loss: 0.3765, Train Acc: 0.8070, Val Acc:0.8276303165973776\n",
      "2022-04-18 13:16:13,861 - wb-bert-fpt-mlm - INFO - [Epoch 1/5] Iteration 150 -> Train Loss: 0.3529, Train Acc: 0.8130, Val Acc:0.8314678605692357\n",
      "2022-04-18 13:16:22,838 - wb-bert-fpt-mlm - INFO - [Epoch 1/5] Iteration 180 -> Train Loss: 0.3302, Train Acc: 0.8141, Val Acc:0.834026223217141\n",
      "2022-04-18 13:16:31,914 - wb-bert-fpt-mlm - INFO - [Epoch 1/5] Iteration 210 -> Train Loss: 0.3138, Train Acc: 0.8184, Val Acc:0.8391429485129517\n",
      "2022-04-18 13:16:40,952 - wb-bert-fpt-mlm - INFO - [Epoch 1/5] Iteration 240 -> Train Loss: 0.2998, Train Acc: 0.8266, Val Acc:0.8423409018228334\n",
      "2022-04-18 13:16:50,042 - wb-bert-fpt-mlm - INFO - [Epoch 1/5] Iteration 270 -> Train Loss: 0.2961, Train Acc: 0.8202, Val Acc:0.8442596738087624\n",
      "2022-04-18 13:16:59,048 - wb-bert-fpt-mlm - INFO - [Epoch 1/5] Iteration 300 -> Train Loss: 0.2790, Train Acc: 0.8279, Val Acc:0.8458586504637032\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ab04285c23460f92e3d2b811b607e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 13:17:08,699 - wb-bert-fpt-mlm - INFO - [Epoch 2/5] Iteration 330 -> Train Loss: 0.2672, Train Acc: 0.8332, Val Acc:0.8468180364566678\n",
      "2022-04-18 13:17:17,627 - wb-bert-fpt-mlm - INFO - [Epoch 2/5] Iteration 360 -> Train Loss: 0.2690, Train Acc: 0.8337, Val Acc:0.8477774224496323\n",
      "2022-04-18 13:17:26,689 - wb-bert-fpt-mlm - INFO - [Epoch 2/5] Iteration 390 -> Train Loss: 0.2678, Train Acc: 0.8348, Val Acc:0.8538535337384074\n",
      "2022-04-18 13:17:35,764 - wb-bert-fpt-mlm - INFO - [Epoch 2/5] Iteration 420 -> Train Loss: 0.2506, Train Acc: 0.8389, Val Acc:0.8551327150623601\n",
      "2022-04-18 13:17:44,807 - wb-bert-fpt-mlm - INFO - [Epoch 2/5] Iteration 450 -> Train Loss: 0.2536, Train Acc: 0.8367, Val Acc:0.8557723057243364\n",
      "2022-04-18 13:17:53,882 - wb-bert-fpt-mlm - INFO - [Epoch 2/5] Iteration 480 -> Train Loss: 0.2509, Train Acc: 0.8373, Val Acc:0.8576910777102654\n",
      "2022-04-18 13:18:02,898 - wb-bert-fpt-mlm - INFO - [Epoch 2/5] Iteration 510 -> Train Loss: 0.2564, Train Acc: 0.8386, Val Acc:0.8583306683722418\n",
      "2022-04-18 13:18:12,052 - wb-bert-fpt-mlm - INFO - [Epoch 2/5] Iteration 540 -> Train Loss: 0.2401, Train Acc: 0.8440, Val Acc:0.8596098496961945\n",
      "2022-04-18 13:18:21,112 - wb-bert-fpt-mlm - INFO - [Epoch 2/5] Iteration 570 -> Train Loss: 0.2437, Train Acc: 0.8436, Val Acc:0.8599296450271826\n",
      "2022-04-18 13:18:30,172 - wb-bert-fpt-mlm - INFO - [Epoch 2/5] Iteration 600 -> Train Loss: 0.2347, Train Acc: 0.8441, Val Acc:0.8596098496961945\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f48b5a2892744728b84908b9a7fac37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 13:18:39,848 - wb-bert-fpt-mlm - INFO - [Epoch 3/5] Iteration 630 -> Train Loss: 0.2388, Train Acc: 0.8467, Val Acc:0.8615286216821234\n",
      "2022-04-18 13:18:48,950 - wb-bert-fpt-mlm - INFO - [Epoch 3/5] Iteration 660 -> Train Loss: 0.2334, Train Acc: 0.8422, Val Acc:0.8605692356891589\n",
      "2022-04-18 13:18:58,025 - wb-bert-fpt-mlm - INFO - [Epoch 3/5] Iteration 690 -> Train Loss: 0.2241, Train Acc: 0.8498, Val Acc:0.8602494403581707\n",
      "2022-04-18 13:19:07,083 - wb-bert-fpt-mlm - INFO - [Epoch 3/5] Iteration 720 -> Train Loss: 0.2408, Train Acc: 0.8424, Val Acc:0.8618484170131117\n",
      "2022-04-18 13:19:16,173 - wb-bert-fpt-mlm - INFO - [Epoch 3/5] Iteration 750 -> Train Loss: 0.2341, Train Acc: 0.8455, Val Acc:0.8621682123440998\n",
      "2022-04-18 13:19:22,377 - wb-bert-fpt-mlm - INFO - ==> save_model : ../model/bert/bmc-fpt-wiki_20190620_mecab_false_0311-nouns-0418/batch:32-ep:5-lr:0.000004151-4m18d-13:19\n",
      "2022-04-18 13:19:26,717 - wb-bert-fpt-mlm - INFO - [Epoch 3/5] Iteration 780 -> Train Loss: 0.2249, Train Acc: 0.8465, Val Acc:0.8624880076750879\n",
      "2022-04-18 13:19:35,736 - wb-bert-fpt-mlm - INFO - [Epoch 3/5] Iteration 810 -> Train Loss: 0.2248, Train Acc: 0.8497, Val Acc:0.8637671889990406\n",
      "2022-04-18 13:19:44,791 - wb-bert-fpt-mlm - INFO - [Epoch 3/5] Iteration 840 -> Train Loss: 0.2176, Train Acc: 0.8547, Val Acc:0.8640869843300287\n",
      "2022-04-18 13:19:53,883 - wb-bert-fpt-mlm - INFO - [Epoch 3/5] Iteration 870 -> Train Loss: 0.2244, Train Acc: 0.8507, Val Acc:0.8647265749920051\n",
      "2022-04-18 13:20:02,989 - wb-bert-fpt-mlm - INFO - [Epoch 3/5] Iteration 900 -> Train Loss: 0.2256, Train Acc: 0.8525, Val Acc:0.864406779661017\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc0f319ed314467b72d33c86075553f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 13:20:12,665 - wb-bert-fpt-mlm - INFO - [Epoch 4/5] Iteration 930 -> Train Loss: 0.2169, Train Acc: 0.8525, Val Acc:0.8640869843300287\n",
      "2022-04-18 13:20:21,767 - wb-bert-fpt-mlm - INFO - [Epoch 4/5] Iteration 960 -> Train Loss: 0.2191, Train Acc: 0.8540, Val Acc:0.864406779661017\n",
      "2022-04-18 13:20:30,855 - wb-bert-fpt-mlm - INFO - [Epoch 4/5] Iteration 990 -> Train Loss: 0.2168, Train Acc: 0.8536, Val Acc:0.864406779661017\n",
      "2022-04-18 13:20:39,362 - wb-bert-fpt-mlm - INFO - [Epoch 4/5] Iteration 1020 -> Train Loss: 0.2128, Train Acc: 0.8538, Val Acc:0.8653661656539815\n",
      "2022-04-18 13:20:48,083 - wb-bert-fpt-mlm - INFO - [Epoch 4/5] Iteration 1050 -> Train Loss: 0.2111, Train Acc: 0.8552, Val Acc:0.8660057563159578\n",
      "2022-04-18 13:20:57,173 - wb-bert-fpt-mlm - INFO - [Epoch 4/5] Iteration 1080 -> Train Loss: 0.2054, Train Acc: 0.8560, Val Acc:0.8660057563159578\n",
      "2022-04-18 13:21:06,204 - wb-bert-fpt-mlm - INFO - [Epoch 4/5] Iteration 1110 -> Train Loss: 0.2167, Train Acc: 0.8554, Val Acc:0.8660057563159578\n",
      "2022-04-18 13:21:15,250 - wb-bert-fpt-mlm - INFO - [Epoch 4/5] Iteration 1140 -> Train Loss: 0.2160, Train Acc: 0.8542, Val Acc:0.8660057563159578\n",
      "2022-04-18 13:21:24,299 - wb-bert-fpt-mlm - INFO - [Epoch 4/5] Iteration 1170 -> Train Loss: 0.2132, Train Acc: 0.8553, Val Acc:0.8656859609849696\n",
      "2022-04-18 13:21:33,352 - wb-bert-fpt-mlm - INFO - [Epoch 4/5] Iteration 1200 -> Train Loss: 0.2159, Train Acc: 0.8550, Val Acc:0.8666453469779342\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6214d38abc4d9f94ac8dff81622cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 13:21:42,888 - wb-bert-fpt-mlm - INFO - [Epoch 5/5] Iteration 1230 -> Train Loss: 0.2153, Train Acc: 0.8522, Val Acc:0.8663255516469459\n",
      "2022-04-18 13:21:51,924 - wb-bert-fpt-mlm - INFO - [Epoch 5/5] Iteration 1260 -> Train Loss: 0.2038, Train Acc: 0.8583, Val Acc:0.8666453469779342\n",
      "2022-04-18 13:22:00,990 - wb-bert-fpt-mlm - INFO - [Epoch 5/5] Iteration 1290 -> Train Loss: 0.2082, Train Acc: 0.8573, Val Acc:0.8669651423089223\n",
      "2022-04-18 13:22:10,055 - wb-bert-fpt-mlm - INFO - [Epoch 5/5] Iteration 1320 -> Train Loss: 0.2102, Train Acc: 0.8567, Val Acc:0.8669651423089223\n",
      "2022-04-18 13:22:19,130 - wb-bert-fpt-mlm - INFO - [Epoch 5/5] Iteration 1350 -> Train Loss: 0.2099, Train Acc: 0.8566, Val Acc:0.8669651423089223\n",
      "2022-04-18 13:22:28,225 - wb-bert-fpt-mlm - INFO - [Epoch 5/5] Iteration 1380 -> Train Loss: 0.2059, Train Acc: 0.8559, Val Acc:0.8669651423089223\n",
      "2022-04-18 13:22:37,324 - wb-bert-fpt-mlm - INFO - [Epoch 5/5] Iteration 1410 -> Train Loss: 0.2045, Train Acc: 0.8598, Val Acc:0.8669651423089223\n",
      "2022-04-18 13:22:46,433 - wb-bert-fpt-mlm - INFO - [Epoch 5/5] Iteration 1440 -> Train Loss: 0.2151, Train Acc: 0.8523, Val Acc:0.8672849376399104\n",
      "2022-04-18 13:22:55,520 - wb-bert-fpt-mlm - INFO - [Epoch 5/5] Iteration 1470 -> Train Loss: 0.2068, Train Acc: 0.8562, Val Acc:0.8669651423089223\n",
      "2022-04-18 13:23:04,592 - wb-bert-fpt-mlm - INFO - [Epoch 5/5] Iteration 1500 -> Train Loss: 0.2009, Train Acc: 0.8608, Val Acc:0.8669651423089223\n",
      "2022-04-18 13:23:13,664 - wb-bert-fpt-mlm - INFO - [Epoch 5/5] Iteration 1530 -> Train Loss: 0.2126, Train Acc: 0.8514, Val Acc:0.8669651423089223\n",
      "2022-04-18 13:23:16,720 - wb-bert-fpt-mlm - INFO - ==> save_model : ../model/bert/bmc-fpt-wiki_20190620_mecab_false_0311-nouns-0418/batch:32-ep:5-lr:0.000004151-4m18d-13:23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Loss</td><td>██▇▆▅▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▁▂▂▃▃▄▃▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇█▇▇▇█████▇█▇</td></tr><tr><td>val_accuracy</td><td>▁▁▂▂▃▄▄▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇██▇███████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Loss</td><td>0.21258</td></tr><tr><td>train_accuracy</td><td>0.85139</td></tr><tr><td>val_accuracy</td><td>0.86697</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">driven-sweep-3</strong>: <a href=\"https://wandb.ai/kobongsoo/bert-wb-mlm-test-2/runs/6g0qns8c\" target=\"_blank\">https://wandb.ai/kobongsoo/bert-wb-mlm-test-2/runs/6g0qns8c</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220418_131518-6g0qns8c/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: li0sdn27 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 2.1054190565895575e-05\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/MOCOMSYS/dev/bong/WandB/wandb/run-20220418_132331-li0sdn27</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kobongsoo/bert-wb-mlm-test-2/runs/li0sdn27\" target=\"_blank\">balmy-sweep-4</a></strong> to <a href=\"https://wandb.ai/kobongsoo/bert-wb-mlm-test-2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/kobongsoo/bert-wb-mlm-test-2/sweeps/d31ha4x0\" target=\"_blank\">https://wandb.ai/kobongsoo/bert-wb-mlm-test-2/sweeps/d31ha4x0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 13:23:38,400 - bwpdataset - INFO - ==>[Start] cached file read: ../korpora/kowiki_20190620/cached_lm_BertTokenizer_128_wiki_20190620_small.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLSid:101, SEPid:102, UNKid:100, PADid:0, MASKid:103\n",
      "*corpus:../korpora/kowiki_20190620/wiki_20190620_small.txt\n",
      "*max_sequence_len:128\n",
      "*mlm_probability:0.15\n",
      "*CLStokenid:101, SEPtokenid:102, UNKtokenid:100, PADtokeinid:0, Masktokeid:103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 13:23:39,086 - bwpdataset - INFO - <==[End] Loading features from cached file ../korpora/kowiki_20190620/cached_lm_BertTokenizer_128_wiki_20190620_small.txt [took 0.685 s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*corpus:../korpora/kowiki_20190620/wiki_eval_test.txt\n",
      "*max_sequence_len:128\n",
      "*mlm_probability:0.15\n",
      "*CLStokenid:101, SEPtokenid:102, UNKtokenid:100, PADtokeinid:0, Masktokeid:103\n",
      "*total_line: 114\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfbb238be5c6487cb3cac9ff825e383e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b178791808c443fbb3b740889c47f19e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc5e03b9a204a35ab4a70e32ea4d2ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "094e7aa09f2b467e9bb467be7b4099d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/614 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 13:23:53,354 - wb-bert-fpt-mlm - INFO - [Epoch 1/4] Iteration 61 -> Train Loss: 0.4318, Train Acc: 0.8010, Val Acc:0.8298688839142948\n",
      "2022-04-18 13:24:03,584 - wb-bert-fpt-mlm - INFO - [Epoch 1/4] Iteration 122 -> Train Loss: 0.3388, Train Acc: 0.8174, Val Acc:0.8381835625199872\n",
      "2022-04-18 13:24:13,783 - wb-bert-fpt-mlm - INFO - [Epoch 1/4] Iteration 183 -> Train Loss: 0.2945, Train Acc: 0.8269, Val Acc:0.857051487048289\n",
      "2022-04-18 13:24:23,988 - wb-bert-fpt-mlm - INFO - [Epoch 1/4] Iteration 244 -> Train Loss: 0.2468, Train Acc: 0.8410, Val Acc:0.8605692356891589\n",
      "2022-04-18 13:24:34,222 - wb-bert-fpt-mlm - INFO - [Epoch 1/4] Iteration 305 -> Train Loss: 0.2293, Train Acc: 0.8443, Val Acc:0.8656859609849696\n",
      "2022-04-18 13:24:44,506 - wb-bert-fpt-mlm - INFO - [Epoch 1/4] Iteration 366 -> Train Loss: 0.2196, Train Acc: 0.8512, Val Acc:0.8714422769427567\n",
      "2022-04-18 13:24:54,574 - wb-bert-fpt-mlm - INFO - [Epoch 1/4] Iteration 427 -> Train Loss: 0.2076, Train Acc: 0.8594, Val Acc:0.8829549088583307\n",
      "2022-04-18 13:25:04,755 - wb-bert-fpt-mlm - INFO - [Epoch 1/4] Iteration 488 -> Train Loss: 0.1909, Train Acc: 0.8691, Val Acc:0.8864726574992006\n",
      "2022-04-18 13:25:14,932 - wb-bert-fpt-mlm - INFO - [Epoch 1/4] Iteration 549 -> Train Loss: 0.1881, Train Acc: 0.8704, Val Acc:0.8877518388231532\n",
      "2022-04-18 13:25:24,989 - wb-bert-fpt-mlm - INFO - [Epoch 1/4] Iteration 610 -> Train Loss: 0.1781, Train Acc: 0.8783, Val Acc:0.8925487687879757\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff82bd43ff1463c9830b4fb4cb14293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/614 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 13:25:35,730 - wb-bert-fpt-mlm - INFO - [Epoch 2/4] Iteration 671 -> Train Loss: 0.1482, Train Acc: 0.8912, Val Acc:0.8957467220978573\n",
      "2022-04-18 13:25:45,824 - wb-bert-fpt-mlm - INFO - [Epoch 2/4] Iteration 732 -> Train Loss: 0.1411, Train Acc: 0.8942, Val Acc:0.8992644707387272\n",
      "2022-04-18 13:25:55,968 - wb-bert-fpt-mlm - INFO - [Epoch 2/4] Iteration 793 -> Train Loss: 0.1336, Train Acc: 0.9009, Val Acc:0.898944675407739\n",
      "2022-04-18 13:26:06,070 - wb-bert-fpt-mlm - INFO - [Epoch 2/4] Iteration 854 -> Train Loss: 0.1409, Train Acc: 0.8996, Val Acc:0.9008634473936681\n",
      "2022-04-18 13:26:16,277 - wb-bert-fpt-mlm - INFO - [Epoch 2/4] Iteration 915 -> Train Loss: 0.1302, Train Acc: 0.9073, Val Acc:0.9037416053725615\n",
      "2022-04-18 13:26:26,475 - wb-bert-fpt-mlm - INFO - [Epoch 2/4] Iteration 976 -> Train Loss: 0.1316, Train Acc: 0.9033, Val Acc:0.9043811960345379\n",
      "2022-04-18 13:26:36,580 - wb-bert-fpt-mlm - INFO - [Epoch 2/4] Iteration 1037 -> Train Loss: 0.1298, Train Acc: 0.9053, Val Acc:0.9056603773584906\n",
      "2022-04-18 13:26:46,759 - wb-bert-fpt-mlm - INFO - [Epoch 2/4] Iteration 1098 -> Train Loss: 0.1306, Train Acc: 0.9063, Val Acc:0.9072593540134314\n",
      "2022-04-18 13:26:56,939 - wb-bert-fpt-mlm - INFO - [Epoch 2/4] Iteration 1159 -> Train Loss: 0.1203, Train Acc: 0.9099, Val Acc:0.9072593540134314\n",
      "2022-04-18 13:27:07,128 - wb-bert-fpt-mlm - INFO - [Epoch 2/4] Iteration 1220 -> Train Loss: 0.1224, Train Acc: 0.9101, Val Acc:0.9059801726894787\n",
      "2022-04-18 13:27:10,167 - wb-bert-fpt-mlm - INFO - ==> save_model : ../model/bert/bmc-fpt-wiki_20190620_mecab_false_0311-nouns-0418/batch:16-ep:4-lr:0.000021054-4m18d-13:27\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ab640d296a42ccb6810d06f11a1609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/614 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 13:27:20,080 - wb-bert-fpt-mlm - INFO - [Epoch 3/4] Iteration 1281 -> Train Loss: 0.1107, Train Acc: 0.9168, Val Acc:0.9072593540134314\n",
      "2022-04-18 13:27:30,295 - wb-bert-fpt-mlm - INFO - [Epoch 3/4] Iteration 1342 -> Train Loss: 0.0982, Train Acc: 0.9241, Val Acc:0.908538535337384\n",
      "2022-04-18 13:27:40,540 - wb-bert-fpt-mlm - INFO - [Epoch 3/4] Iteration 1403 -> Train Loss: 0.0988, Train Acc: 0.9258, Val Acc:0.9069395586824432\n",
      "2022-04-18 13:27:50,755 - wb-bert-fpt-mlm - INFO - [Epoch 3/4] Iteration 1464 -> Train Loss: 0.1044, Train Acc: 0.9238, Val Acc:0.9094979213303486\n",
      "2022-04-18 13:28:00,947 - wb-bert-fpt-mlm - INFO - [Epoch 3/4] Iteration 1525 -> Train Loss: 0.0935, Train Acc: 0.9273, Val Acc:0.9088583306683723\n",
      "2022-04-18 13:28:11,156 - wb-bert-fpt-mlm - INFO - [Epoch 3/4] Iteration 1586 -> Train Loss: 0.0964, Train Acc: 0.9257, Val Acc:0.9094979213303486\n",
      "2022-04-18 13:28:21,350 - wb-bert-fpt-mlm - INFO - [Epoch 3/4] Iteration 1647 -> Train Loss: 0.0962, Train Acc: 0.9259, Val Acc:0.9098177166613367\n",
      "2022-04-18 13:28:31,526 - wb-bert-fpt-mlm - INFO - [Epoch 3/4] Iteration 1708 -> Train Loss: 0.1019, Train Acc: 0.9237, Val Acc:0.9098177166613367\n",
      "2022-04-18 13:28:41,700 - wb-bert-fpt-mlm - INFO - [Epoch 3/4] Iteration 1769 -> Train Loss: 0.0957, Train Acc: 0.9267, Val Acc:0.9104573073233131\n",
      "2022-04-18 13:28:51,847 - wb-bert-fpt-mlm - INFO - [Epoch 3/4] Iteration 1830 -> Train Loss: 0.0994, Train Acc: 0.9265, Val Acc:0.910137511992325\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5498f23f2e074fa1af15ecd20fa1d9c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/614 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 13:29:02,681 - wb-bert-fpt-mlm - INFO - [Epoch 4/4] Iteration 1891 -> Train Loss: 0.0826, Train Acc: 0.9358, Val Acc:0.9094979213303486\n",
      "2022-04-18 13:29:12,804 - wb-bert-fpt-mlm - INFO - [Epoch 4/4] Iteration 1952 -> Train Loss: 0.0816, Train Acc: 0.9379, Val Acc:0.9094979213303486\n",
      "2022-04-18 13:29:22,948 - wb-bert-fpt-mlm - INFO - [Epoch 4/4] Iteration 2013 -> Train Loss: 0.0882, Train Acc: 0.9347, Val Acc:0.9098177166613367\n",
      "2022-04-18 13:29:33,043 - wb-bert-fpt-mlm - INFO - [Epoch 4/4] Iteration 2074 -> Train Loss: 0.0831, Train Acc: 0.9347, Val Acc:0.910137511992325\n",
      "2022-04-18 13:29:43,254 - wb-bert-fpt-mlm - INFO - [Epoch 4/4] Iteration 2135 -> Train Loss: 0.0823, Train Acc: 0.9352, Val Acc:0.9107771026543012\n",
      "2022-04-18 13:29:53,436 - wb-bert-fpt-mlm - INFO - [Epoch 4/4] Iteration 2196 -> Train Loss: 0.0803, Train Acc: 0.9367, Val Acc:0.9104573073233131\n",
      "2022-04-18 13:30:03,641 - wb-bert-fpt-mlm - INFO - [Epoch 4/4] Iteration 2257 -> Train Loss: 0.0860, Train Acc: 0.9315, Val Acc:0.9114166933162776\n",
      "2022-04-18 13:30:13,759 - wb-bert-fpt-mlm - INFO - [Epoch 4/4] Iteration 2318 -> Train Loss: 0.0825, Train Acc: 0.9347, Val Acc:0.9107771026543012\n",
      "2022-04-18 13:30:23,942 - wb-bert-fpt-mlm - INFO - [Epoch 4/4] Iteration 2379 -> Train Loss: 0.0838, Train Acc: 0.9350, Val Acc:0.9110968979852894\n",
      "2022-04-18 13:30:34,057 - wb-bert-fpt-mlm - INFO - [Epoch 4/4] Iteration 2440 -> Train Loss: 0.0851, Train Acc: 0.9341, Val Acc:0.9110968979852894\n",
      "2022-04-18 13:30:38,198 - wb-bert-fpt-mlm - INFO - ==> save_model : ../model/bert/bmc-fpt-wiki_20190620_mecab_false_0311-nouns-0418/batch:16-ep:4-lr:0.000021054-4m18d-13:30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Loss</td><td>█▆▅▄▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▂▂▃▃▄▄▄▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>val_accuracy</td><td>▁▂▃▄▄▅▆▆▆▆▇▇▇▇▇▇████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Loss</td><td>0.08515</td></tr><tr><td>train_accuracy</td><td>0.9341</td></tr><tr><td>val_accuracy</td><td>0.9111</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">balmy-sweep-4</strong>: <a href=\"https://wandb.ai/kobongsoo/bert-wb-mlm-test-2/runs/li0sdn27\" target=\"_blank\">https://wandb.ai/kobongsoo/bert-wb-mlm-test-2/runs/li0sdn27</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220418_132331-li0sdn27/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 훈련을 시작 (총 3번(epoch이 아님))\n",
    "wandb.agent(sweep_id, train, count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111fac71-a6d8-4eb6-bc77-e1bca6b19225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
